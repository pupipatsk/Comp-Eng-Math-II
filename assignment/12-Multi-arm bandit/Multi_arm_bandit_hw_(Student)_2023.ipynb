{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye6JQHc73J5S"
      },
      "source": [
        "# A/B Testing from Scratch: Multi-armed Bandits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmS6wcXx3J5S"
      },
      "source": [
        "Frequentist and Bayesian A/B tests require you to divide your traffic into arbitrary groups for a period of time, then perform statistical tests based on the results. By definition, this forces us to divert out traffic to suboptimal variations during the test period, resulting in lower overall conversion rates. On the other hand, multi-barmed bandit appraoch (MAB) dynamically adjusts the percentage of traffic shown to each variation according to how they have performed so far during the test, resulting in smaller loss in conversion rates.\n",
        "\n",
        "![Traditional A/B Test vs Multi-armed Bandits](https://github.com/cstorm125/abtestoo/blob/master/images/ab_vs_mab.png?raw=true)\n",
        "\n",
        "Source: [Automizy](https://automizy.com/blog/increase-email-course-open-rates-with-machine-learning/) via [Multi-Arm Bandits: a potential alternative to A/B tests](https://medium.com/brillio-data-science/multi-arm-bandits-a-potential-alternative-to-a-b-tests-a647d9bf2a7e)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGTZ4IN73J5S"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "#plots\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import *\n",
        "\n",
        "#stats\n",
        "import scipy as sp\n",
        "import statsmodels as sm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZiK1eIq3J5T"
      },
      "source": [
        "## Arms, Variations, Ads, or Anything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPV1Kcdu3J5T"
      },
      "source": [
        "We treat serving a variation of content, be it product listings, recommended products, search results, online ads, or whatever we want to experiment on as *pulling an arm*. The arm will record an impression and, at an arbitrary amount of delay time, an action such as a click or add-to-cart based on that impression. In our example, we define our arm as a Bernoulli trial with the true probability of conversion ($\\frac{actions}{impressions}$) as `true_p`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ouRkfk83J5T"
      },
      "source": [
        "class Arm:\n",
        "    def __init__(self, true_p):\n",
        "        self.true_p = true_p\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.impressions = 0\n",
        "        self.actions = 0\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.impressions, self.actions\n",
        "\n",
        "    def get_rate(self):\n",
        "        return self.actions / self.impressions if self.impressions >0 else 0.\n",
        "\n",
        "    def pull(self):\n",
        "        self.impressions+=1\n",
        "        res = 1 if np.random.random() < self.true_p else 0\n",
        "        self.actions+=res\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD6esJGk3J5T"
      },
      "source": [
        "a = Arm(0.1)\n",
        "for i in range(100): a.pull()\n",
        "a.get_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js12kez83J5T"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29ccgwj3J5T"
      },
      "source": [
        "We simulate an environment is an arbitrary number of arms with a set of predefined true probability `true_p` and average number of rewards `avg_rewards` per time period `t`. This environment mimics most content serving APIs which display each variation at the ratio `ps` as defined by experimenters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6xWytk3J5T"
      },
      "source": [
        "class MusketeerEnv:\n",
        "    def __init__(self, true_ps, avg_impressions):\n",
        "        self.true_ps = true_ps\n",
        "        self.avg_impressions = avg_impressions\n",
        "        self.nb_arms = len(true_ps)\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = -1\n",
        "        self.ds=[]\n",
        "        self.arms = [Arm(p) for p in self.true_ps]\n",
        "        return self.get_state()\n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        return [self.arms[i].get_state() for i in range(self.nb_arms)]\n",
        "\n",
        "\n",
        "    def get_rates(self):\n",
        "        return [self.arms[i].get_rate() for i in range(self.nb_arms)]\n",
        "\n",
        "\n",
        "    # sample the actual number of impressions from a triangular function\n",
        "    def get_impressions(self):\n",
        "        return int(np.random.triangular(self.avg_impressions/2,\n",
        "                                    self.avg_impressions,\n",
        "                                    self.avg_impressions*1.5))\n",
        "\n",
        "    # ramdomly choose arm based on a given probabiliy `ps`\n",
        "    def step(self, ps):\n",
        "        self.t += 1\n",
        "        impressions = self.get_impressions()\n",
        "        for i in np.random.choice(a=self.nb_arms,size=impressions,p=ps):\n",
        "            self.arms[i].pull()\n",
        "        self.record()\n",
        "        return self.get_state()\n",
        "\n",
        "\n",
        "    # for logging\n",
        "    def record(self):\n",
        "        d = {'t':self.t, 'max_rate':0, 'opt_impressions':0}\n",
        "\n",
        "        for i in range(self.nb_arms):\n",
        "            d[f'impressions_{i}'],d[f'actions_{i}'] = self.arms[i].get_state()\n",
        "            d[f'rate_{i}'] = self.arms[i].get_rate()\n",
        "\n",
        "            if d[f'rate_{i}'] > d['max_rate']:\n",
        "                d['max_rate'] = d[f'rate_{i}']\n",
        "                d['opt_impressions'] = d[f'impressions_{i}']\n",
        "\n",
        "\n",
        "        d['total_impressions'] = sum([self.arms[i].impressions for i in range(self.nb_arms)])\n",
        "        d['opt_impressions_rate'] = d['opt_impressions'] / d['total_impressions']\n",
        "        d['total_actions'] = sum([self.arms[i].actions for i in range(self.nb_arms)])\n",
        "        d['total_rate'] = d['total_actions'] / d['total_impressions']\n",
        "        d['regret_rate'] = d['max_rate'] - d['total_rate']\n",
        "        d['regret'] = d['regret_rate'] * d['total_impressions']\n",
        "        self.ds.append(d)\n",
        "\n",
        "\n",
        "    # for printting\n",
        "    def show_df(self):\n",
        "        df = pd.DataFrame(self.ds)\n",
        "        cols = ['t'] + [f'rate_{i}' for i in range(self.nb_arms)]+ \\\n",
        "               [f'impressions_{i}' for i in range(self.nb_arms)]+ \\\n",
        "               [f'actions_{i}' for i in range(self.nb_arms)]+ \\\n",
        "               ['total_impressions','total_actions','total_rate']+ \\\n",
        "               ['opt_impressions','opt_impressions_rate']+ \\\n",
        "               ['regret_rate','regret']\n",
        "        df = df[cols]\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAA1ERpF3J5T"
      },
      "source": [
        "For instance, in a traditional A/B test with a default variation, new variation `A` and new variation `B`. We may divide 60% traffic to the default variation and 20% each to `A` and `B`. After 1,000 time steps, we will get the following results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0PwVLya3J5T"
      },
      "source": [
        "env = MusketeerEnv(true_ps = [0.1,0.12,0.13], avg_impressions=400)\n",
        "for i in range(1000):\n",
        "    env.step([0.6,0.2,0.2])\n",
        "env.get_rates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK01Q-Xh3J5T"
      },
      "source": [
        "In order to evaluate an MAB agent, we use 3 main metrics:\n",
        "1. `opt_impressions_rate`: cumulative percentage of impressions we have given to the optimal arm at that timestep; this shows us how often we have picked the \"best\" arm\n",
        "2. `regret_rate`: cumulative conversion rate of the best arm at that timestep minus cumulative conversion rate of all impressions; this shows us the difference in conversion rate we have lost by not picking the \"best\" arm\n",
        "3. `regret`: cumulative actions if we had chosen the \"best\" arm minus actual cumulative conversions; this shows us how much actions we have lost by not picking the \"best\" arm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrEQ8PEE3J5U"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWx9TMn63J5U"
      },
      "source": [
        "An MAB agent solves the explore-vs-exploit dilemma. Exploitation means we choose what we know as the best choice at the current timestep, sometimes called being *greedy*; on the other hand, exploration means we try pulling other arms in order to know more about the environment.\n",
        "\n",
        "Exploiting 100% of the time is a bad idea. For instance; let us assume there are two arms `A` and `B` with true probabilities 0.1 and 0.9 and it happens that when we pull `A` it returns a conversion whereas when we pull `B` it does not. If our policy is to always exploit, we would end up pulling only `A` which has much lower return rate than `B`. This is when you do not have any experiment set up for your content at all.\n",
        "\n",
        "In contrast, if we always explore, we would end up pulling both arms randomly with expected return rates of $0.9 * 0.5 + 0.1 * 0.5 = 0.5$ instead of much higher if we could find out `B` is the better arm. This is close to what happens in a traditional A/B test during the test period.\n",
        "\n",
        "Some common policies for distributing impressions to each arm are:\n",
        "1. **Equal weights**: all arms have the same amount of traffic or a fixed amount.\n",
        "2. **Randomize**: randomly assign traffic to all arms.\n",
        "3. **Epsilon-greedy**: Assign a majority of traffic to the \"best\" arm at that time step, and the rest randomized among all arms; the degree of random traffic can be decayed by a parameter `gamma` as time goes on.\n",
        "4. **Softmax or Boltzmann exploration**: Assigns traffic equal to the softmax activation of their current return rates; regulated by temperature parameter `tau` (lower `tau` means less exploration) that can also be decayed by `gamma` over time.\n",
        "$$P(A_i) = \\frac{e^{rate_i/\\tau}}{\\sum{e^{rate_i/\\tau}}}$$\n",
        "\n",
        "5. **Upper Confidence Bound**: by utilizing Hoeffdingâ€™s Inequality, we can have a deterministic policy based on number of times the arms are pulled so far and impressions of each arm:\n",
        "\n",
        "$$A = argmax(rate_i + \\sqrt{\\frac{2\\log{t}}{impressions_i}})$$\n",
        "\n",
        "6. **Deterministic Thompson Sampling**: based on a posterior distribution (in our case a Beta distribution) for each arm, sample that number of rates. Choose the arm with the highest sampled rate.\n",
        "\n",
        "7. **Stochastic Thompson Sampling**: Instead of sampling only once, perform a Monte Carlo simulation for an arbitrary number of times, the traffic to each arm is divided by the percentage of times that arm is the best arm in the simulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErFkVTSz3J5U"
      },
      "source": [
        "class BanditAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    #baselines\n",
        "    def equal_weights(self,state):\n",
        "        p_actions = np.array([1/len(state) for i in range(len(state))])\n",
        "        return p_actions\n",
        "\n",
        "\n",
        "    # TODO:1 : write a function that give the probability of choosing arm randomly\n",
        "    def randomize(self,state):\n",
        "        pass\n",
        "        # CODE HERE\n",
        "        #\n",
        "        # return p_actions\n",
        "\n",
        "\n",
        "    # TODO:2 : write a function that give the probability of choosing arm based on epsilon greedy policy\n",
        "    def eps_greedy(self, state, t, start_eps=0.3, end_eps=0.01, gamma=0.99):\n",
        "        pass\n",
        "        # CODE HERE\n",
        "        #\n",
        "        # return p_actions\n",
        "\n",
        "\n",
        "    # TODO:3 : write a function that give the probability of choosing arm based on softmax greedy policy\n",
        "    def softmax(self, state, t, start_tau=1e-1, end_tau=1e-4, gamma=0.9):\n",
        "        pass\n",
        "        # CODE HERE\n",
        "        #\n",
        "        # return p_actions\n",
        "\n",
        "\n",
        "    # TODO:4 : write a function that give the probability of choosing arm based on UCB policy\n",
        "    def ucb(self, state, t):\n",
        "        pass\n",
        "        # CODE HERE\n",
        "        #\n",
        "        # return p_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EigAg3Ga3J5U"
      },
      "source": [
        "## Simulation Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jHw-iEmlebk"
      },
      "source": [
        "We simulate 4 campaigns with true probabilities of 12%, 13%, 15% and, 16% respectively. Our number of overall impressions is 400 on average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtdMX9k33J5U"
      },
      "source": [
        "env = MusketeerEnv(true_ps = [0.12,0.13,0.15,0.16], avg_impressions=400)\n",
        "a = BanditAgent()\n",
        "for i in range(20):\n",
        "    p = a.equal_weights(env.get_state())\n",
        "    env.step(p)\n",
        "    t=i\n",
        "a.equal_weights(env.get_state()), a.randomize(env.get_state()), a.eps_greedy(env.get_state(),t), \\\n",
        "a.softmax(env.get_state(),t), a.ucb(env.get_state(),t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9uKT3NqibFg"
      },
      "source": [
        "N_policy = 5\n",
        "envs = [MusketeerEnv(true_ps = [0.12,0.13,0.15,0.16], avg_impressions=400) for i in range(N_policy)]\n",
        "a = BanditAgent()\n",
        "for t in range(250):\n",
        "    states = [env.get_state() for env in envs]\n",
        "    actions = [a.equal_weights(states[0]), a.randomize(states[1]),\n",
        "               a.eps_greedy(states[2],t), a.softmax(states[3],t),\n",
        "               a.ucb(states[4],t)]\n",
        "\n",
        "    for i in range(N_policy): envs[i].step(actions[i])\n",
        "\n",
        "dfs = [env.show_df() for env in envs]\n",
        "policies = ['equal_weights','randomize','eps_greedy','softmax','ucb']\n",
        "for i in range(N_policy): dfs[i]['policy'] = policies[i]\n",
        "df = pd.concat(dfs)[['policy','t','opt_impressions_rate','regret_rate', 'regret', 'total_rate']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37NTe5m1i1fH"
      },
      "source": [
        "df_m = df.melt(id_vars=['policy','t'])\n",
        "df_m.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rma3OLLQ3J5U"
      },
      "source": [
        "g = (ggplot(df_m, aes(x='t', y='value', color='policy', group='policy')) +\n",
        "    geom_line() + theme_minimal() + facet_wrap('~variable', scales='free_y'))\n",
        "g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mp71c7TzGJ7"
      },
      "source": [
        "TODO:5 Compare the result. Which policy has the best performance ?\n",
        "\n",
        "\n",
        "**Submit a pdf with screenshots of TODO1-5.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSbzWUtP3J5U"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MTjc3sl3J5U"
      },
      "source": [
        "Here are some useful resources reviewed for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpWEMvie3J5U"
      },
      "source": [
        "- [tl;dr Bayesian A/B test](https://medium.com/hockey-stick/tl-dr-bayesian-a-b-testing-with-python-c495d375db4d)\n",
        "- [Bayesian A/B Testing: a step-by-step guide](http://www.claudiobellei.com/2017/11/02/bayesian-AB-testing/)\n",
        "- [Bayesian Coin Flips](https://www.thomasjpfan.com/2015/09/bayesian-coin-flips/)\n",
        "- [Multi-Arm Bandits: a potential alternative to A/B tests](https://medium.com/brillio-data-science/multi-arm-bandits-a-potential-alternative-to-a-b-tests-a647d9bf2a7e)\n",
        "- [Multi Armed Bandits and Exploration Strategies](https://sudeepraja.github.io/Bandits/)\n",
        "- [MAB Google](https://support.google.com/analytics/answer/2846882?hl=en)"
      ]
    }
  ]
}